#  Ethiopian Medical Business Data Pipeline & Analytics

## ğŸš€ Project Overview

This project implements a complete end-to-end **data pipeline** to extract, process, enrich, and serve medical business data from **Ethiopian Telegram channels**. It integrates **modern data engineering tools**, enabling analytical exploration and AI-powered insights via a robust API.

The final output is an enriched, queryable **PostgreSQL data warehouse**â€”ideal for data science, reporting, and business intelligence.

---

## ğŸ§° Tech Stack Overview

| Layer             | Tool/Framework              | Purpose                                       |
| ----------------- | --------------------------- | --------------------------------------------- |
| ğŸ“¥ Data Ingestion | `Telegram API` (via Python) | Extract chat messages and media from channels |
| ğŸ“‚ Data Lake      | Local `.json` files         | Raw data storage                              |
| ğŸ§± Data Warehouse | `PostgreSQL`                | Structured storage for analytics              |
| ğŸ§¹ Transformation | `dbt` (Data Build Tool)     | Clean, model, and test data                   |
| ğŸ§  Enrichment     | `YOLOv8` (Ultralytics)      | Detect objects in shared images using AI      |
| ğŸŒ API Access     | `FastAPI`                   | Serve insights via RESTful endpoints          |
| â±ï¸ Orchestration  | `Dagster`                   | Schedule and monitor pipeline jobs            |

---

## ğŸ“¦ dbt: Modular, Auditable Transformations

This project uses [**dbt**](https://www.getdbt.com/) to transform raw scraped data into trusted, analytics-ready models.

### ğŸ§  Features:

* Modular SQL transformations with Jinja
* Built-in testing: `not_null`, `unique`, `relationships`
* Auto-generated documentation (`dbt docs`)
* Reproducible and version-controlled

### ğŸ“ Model Layers:

* `staging/` 
* `marts/`
* `fact/`

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ .github/workflows/               # CI/CD via GitHub Actions
â”œâ”€â”€ analytical-api/                 # FastAPI for analytics endpoints
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ crud.py
â”œâ”€â”€ dagster_jobs/                   # Dagster orchestration jobs
â”‚   â”œâ”€â”€ jobs.py
â”‚   â”œâ”€â”€ schedules.py
â”‚   â””â”€â”€ ops/
â”‚       â”œâ”€â”€ scrape.py
â”‚       â”œâ”€â”€ load.py
â”‚       â”œâ”€â”€ transform.py
â”‚       â””â”€â”€ enrich.py
â”œâ”€â”€ dbt/telegram_warehouse/         # dbt project: models, sources, tests
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ fact/
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ sources.yml
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ notebook/                       # Jupyter notebooks for EDA & testing
â”‚   â”œâ”€â”€ 1.0-telegram-scraper.ipynb
â”‚   â””â”€â”€ 3.0-image-detection.ipynb
â”‚   â””â”€â”€ yolo_enrichment.py
â”œâ”€â”€ script/                         # Standalone utility scripts
â”‚   â”œâ”€â”€ scrape_telegram.py
â”‚   â”œâ”€â”€ load_json_to_postgres.py
â”‚   â””â”€â”€ detect_objects_yolo.py
â”œâ”€â”€ src/                            # Core logic (OOP)
â”‚   â””â”€â”€ telegram_scraper.py
â”œâ”€â”€ .env                            # Environment variables (API keys, DB)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml                  # Dagster config (via `[tool.dagster]`)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Getting Started

### âœ… Prerequisites

* Python 3.8+
* PostgreSQL (e.g., v13+)
* Telegram API credentials (`api_id`, `api_hash`)

---

### ğŸ§ª Installation & Setup

```bash
# 1. Clone the repository
git clone git@github.com:kumsa-Mergia/Telegram-Data-to-API.git
cd Telegram-Data-to-API

# 2. Create and activate a virtual environment
python -m venv venv
source venv/bin/activate         # Linux/macOS
# .\venv\Scripts\activate        # Windows

# 3. Install dependencies
pip install -r requirements.txt
```

---

### ğŸ› ï¸ PostgreSQL Setup

1. Create a new database named `telegram_db`
2. Create two schemas:

   * `raw` for raw Telegram and YOLO data
   * `analytics` for dbt models

---

### ğŸ§© dbt Setup

```bash
# Navigate to dbt project directory
cd dbt/telegram_warehouse/

# Create your dbt profile (~/.dbt/profiles.yml)
# Then run the following:
dbt debug           # Validate profile
dbt build           # Run models and tests
dbt docs generate   # Generate documentation
dbt docs serve      # Open docs on localhost
```
## ğŸ–¼ï¸ Screenshot  Open docs on localhost
![docs UI Screenshot](https://drive.google.com/uc?id=1_1jGMeuUkmxi6TEJarTHDdYkeurDbT01)


### ğŸš€ Run the Analytical API (FastAPI)

The API exposes YOLO-enriched message data and analytics from PostgreSQL for consumption by dashboards or other services.

```bash
# Navigate to the API directory
cd analytical-api

# Run the FastAPI server
uvicorn main:app --reload
```

* The API will be available at: [http://127.0.0.1:8000](http://127.0.0.1:8000)
* Interactive API docs at: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
## ğŸ–¼ï¸ Screenshot   Analytical API (FastAPI)
![FastAPI UI Screenshot](https://drive.google.com/uc?id=13Bv52zCdfmXCyC8gjmaaxVOZYzy4UcQ-)



### ğŸŒ€ Dagster Setup & Orchestration

```bash
# Run Dagster UI from the project root
dagster dev -w workspace.yaml
```

This command:

* Launches the **Dagster webserver** and **daemon** for local development.
* Loads pipeline definitions from your `workspace.yaml` configuration.
* Exposes the orchestration dashboard at:
  [http://127.0.0.1:3000](http://127.0.0.1:3000)

**Tip**: Make sure youâ€™ve defined your `@job`s inside `dagster_jobs/jobs.py` and pointed to it correctly in your `workspace.yaml`.

---
## ğŸ–¼ï¸ Screenshot Dagster UI
![Dagster UI Screenshot](https://drive.google.com/uc?id=1N5cGdi5xV5BWAj6iuYpS8flCPyGqNGhD)

---

## ğŸ‘¨â€ğŸ’» Author

**Kumsa Mergia**
ğŸ”— [GitHub](https://github.com/kumsa-Mergia) | ğŸŒ Ethiopia

---

